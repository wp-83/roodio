{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93fab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/26 23:23:56 INFO mlflow.tracking.fluent: Experiment with name 'Roodio_BERT_Finetuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Working Directory: c:\\CAWU4GROUP3\\projects\\projectRoodio\\machineLearning\\src\\models\\nlpModel\n",
      "üì° MLflow Tracking URI: file:///c:/CAWU4GROUP3/projects/projectRoodio/machineLearning/mlruns\n",
      "üíæ Model akan disimpan di: c:\\CAWU4GROUP3\\projects\\projectRoodio\\machineLearning\\mlruns\\final_model_bert\n",
      "üß™ Experiment Name: Roodio_BERT_Finetuning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import torch\n",
    "import pathlib\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- KONFIGURASI PATH ---\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Target Utama: root/mlruns\n",
    "# (Naik 3 level dari root/src/models/nlpModel ke root, lalu masuk mlruns)\n",
    "mlflow_dir = os.path.abspath(os.path.join(current_dir, \"../../../mlruns\"))\n",
    "\n",
    "# Target Output Model: root/mlruns/final_model_bert\n",
    "# (Kita simpan hasil save model langsung di dalam folder mlruns juga biar ngumpul)\n",
    "output_dir = os.path.join(mlflow_dir, \"final_model_bert\")\n",
    "\n",
    "LOCAL_TEST_FILE = os.path.abspath(os.path.join(current_dir, \"../../../data/lyrics/lyrics.csv\")) \n",
    "\n",
    "# Buat folder mlruns dan output jika belum ada\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "\n",
    "# --- 2. SETUP URI (Windows Pathlib Fix) ---\n",
    "# Mengubah \"C:\\...\" menjadi \"file:///C:/...\" agar MLflow tidak error\n",
    "mlflow_tracking_uri = pathlib.Path(mlflow_dir).as_uri()\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# --- 3. SETUP EXPERIMENT ---\n",
    "experiment_name = \"Roodio_BERT_Finetuning\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üìÇ Working Directory: {current_dir}\")\n",
    "print(f\"üì° MLflow Tracking URI: {mlflow_tracking_uri}\")\n",
    "print(f\"üíæ Model akan disimpan di: {output_dir}\")\n",
    "print(f\"üß™ Experiment Name: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7fa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Menyiapkan Dataset GoEmotions...\n",
      "‚úÖ Data Mentah Terload: 43410 baris.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels       id\n",
       "0  My favourite food is anything I didn't have to...   [27]  eebbqej\n",
       "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"‚¨áÔ∏è Menyiapkan Dataset GoEmotions...\")\n",
    "ds = load_dataset(\"go_emotions\", \"simplified\")\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Data Mentah Terload: {len(df)} baris.\")\n",
    "display(df.head(3)) # Menampilkan sampel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebedaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Melakukan Mapping & Balancing...\n",
      "‚úÖ Data Training Siap: 10396 baris.\n",
      "‚öñÔ∏è Jumlah per kelas: 2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andiz\\AppData\\Local\\Temp\\ipykernel_27608\\3753455655.py:28: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df_clean.groupby('label_name').apply(lambda x: x.sample(n=min_sample, random_state=26)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Definisi Mapping\n",
    "target_map = {\n",
    "    'anger': 'angry', 'annoyance': 'angry', 'disapproval': 'angry', 'fear': 'angry', 'nervousness': 'angry',\n",
    "    'joy': 'happy', 'excitement': 'happy', 'love': 'happy', 'admiration': 'happy', 'amusement': 'happy', 'optimism': 'happy',\n",
    "    'sadness': 'sad', 'disappointment': 'sad', 'grief': 'sad', 'remorse': 'sad',\n",
    "    'relief': 'relaxed', 'neutral': 'relaxed', 'realization': 'relaxed'\n",
    "}\n",
    "labels_list = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \n",
    "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \n",
    "    \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \n",
    "    \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\", \"realization\", \n",
    "    \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "def map_emotion(row_labels):\n",
    "    for label_idx in row_labels:\n",
    "        lbl = labels_list[label_idx]\n",
    "        if lbl in target_map: return target_map[lbl]\n",
    "    return None\n",
    "\n",
    "print(\"‚öôÔ∏è Melakukan Mapping & Balancing...\")\n",
    "df['label_name'] = df['labels'].apply(map_emotion)\n",
    "df_clean = df.dropna(subset=['label_name']).copy()\n",
    "\n",
    "# 2. Undersampling (Agar Adil)\n",
    "min_sample = df_clean['label_name'].value_counts().min()\n",
    "df_balanced = df_clean.groupby('label_name').apply(lambda x: x.sample(n=min_sample, random_state=26)).reset_index(drop=True)\n",
    "\n",
    "# 3. Finalisasi Dataset\n",
    "label2id = {'angry': 0, 'happy': 1, 'relaxed': 2, 'sad': 3}\n",
    "id2label = {0: 'angry', 1: 'happy', 2: 'relaxed', 3: 'sad'}\n",
    "df_balanced['label'] = df_balanced['label_name'].map(label2id)\n",
    "\n",
    "# Buat Dataset HuggingFace\n",
    "train_dataset = Dataset.from_pandas(df_balanced[['text', 'label']].rename(columns={\"text\": \"lyrics\"}))\n",
    "\n",
    "print(f\"‚úÖ Data Training Siap: {len(train_dataset)} baris.\")\n",
    "print(f\"‚öñÔ∏è Jumlah per kelas: {min_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7cce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Membaca file lirik lokal: c:\\CAWU4GROUP3\\projects\\projectRoodio\\machineLearning\\data\\lyrics\\lyrics.csv\n",
      "‚úÖ Data Test Siap: 100 lagu.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(LOCAL_TEST_FILE):\n",
    "    print(f\"üìÇ Membaca file lirik lokal: {LOCAL_TEST_FILE}\")\n",
    "    \n",
    "    raw_test_data = []\n",
    "    current_buffer = []\n",
    "    valid_moods = ['angry', 'happy', 'relaxed', 'sad']\n",
    "    \n",
    "    with open(LOCAL_TEST_FILE, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = lines[1:] # Skip header\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        is_end = False\n",
    "        if ';' in line:\n",
    "            parts = line.rsplit(';', 1)\n",
    "            if len(parts) == 2:\n",
    "                pmood = parts[1].lower().strip()\n",
    "                if pmood in valid_moods:\n",
    "                    full_lyric = \" \".join(current_buffer) + \" \" + parts[0]\n",
    "                    # Cleaning simpel\n",
    "                    full_lyric = re.sub(r'\\[.*?\\]', '', full_lyric)\n",
    "                    full_lyric = re.sub(r\"[^a-z\\s']\", '', full_lyric.lower())\n",
    "                    raw_test_data.append({'lyrics': full_lyric, 'label': label2id[pmood]})\n",
    "                    current_buffer = []\n",
    "                    is_end = True\n",
    "        if not is_end:\n",
    "            current_buffer.append(line)\n",
    "    \n",
    "    test_dataset = Dataset.from_pandas(pd.DataFrame(raw_test_data))\n",
    "    print(f\"‚úÖ Data Test Siap: {len(test_dataset)} lagu.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå WARNING: File {LOCAL_TEST_FILE} tidak ditemukan di {current_dir}.\")\n",
    "    print(\"‚ö†Ô∏è Training akan berjalan menggunakan dummy test set (sebagian dari data train).\")\n",
    "    test_dataset = train_dataset.select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a86334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Tokenizer & Model: bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andiz\\anaconda3\\envs\\roodio-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andiz\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [00:00<00:00, 805.24it/s, Materializing param=bert.pooler.dense.weight]                               \n",
      "BertForSequenceClassification LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Melakukan Tokenisasi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10396/10396 [00:00<00:00, 18279.67 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3793.35 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenisasi Selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "print(f\"‚è≥ Loading Tokenizer & Model: {model_ckpt}...\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, \n",
    "    num_labels=4, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    # Max length 128 cukup untuk GoEmotions\n",
    "    return tokenizer(examples[\"lyrics\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "print(\"‚öôÔ∏è Melakukan Tokenisasi...\")\n",
    "tokenized_train = train_dataset.map(tokenize_func, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_func, batched=True)\n",
    "\n",
    "print(\"‚úÖ Tokenisasi Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ba6812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ MEMULAI TRAINING...\n",
      "‚ö†Ô∏è Estimasi waktu: 30 menit - 2 jam (tergantung kecepatan CPU).\n",
      "‚òï Silakan buat kopi...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1950' max='1950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1950/1950 3:12:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.893313</td>\n",
       "      <td>1.973003</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.224946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605922</td>\n",
       "      <td>1.941605</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.271711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.515543</td>\n",
       "      <td>2.257417</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.262954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.47it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Training Selesai! Model disimpan di: c:\\CAWU4GROUP3\\projects\\projectRoodio\\machineLearning\\mlruns\\final_model_bert\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    f1 = f1_score(labels, pred, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Konfigurasi Training\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    \n",
    "    # Batch Size Aman untuk CPU\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"mlflow\",\n",
    "    run_name=\"Local_CPU_BERT_GoEmotions\",\n",
    "    \n",
    "    # --- CPU SETTINGS ---\n",
    "    use_cpu=True,  # Pastikan ini True\n",
    ")\n",
    "\n",
    "# Inisialisasi Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    \n",
    "    # --- PERBAIKAN DI SINI ---\n",
    "    # Dulu: tokenizer=tokenizer\n",
    "    # Sekarang: processing_class=tokenizer\n",
    "    processing_class=tokenizer, \n",
    "    \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ MEMULAI TRAINING...\")\n",
    "print(\"‚ö†Ô∏è Estimasi waktu: 30 menit - 2 jam (tergantung kecepatan CPU).\")\n",
    "print(\"‚òï Silakan buat kopi...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\"\\nüíæ Training Selesai! Model disimpan di: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roodio-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
